{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results Analysis\n",
    "\n",
    "Analyze and visualize LLM evaluation results from the harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path().absolute().parent / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from llm_eval.metrics.storage import MetricsStorage\n",
    "from llm_eval.metrics.exporters import CSVExporter\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Recent Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage\n",
    "storage = MetricsStorage('../data/metrics/eval_results.db')\n",
    "\n",
    "# List recent runs\n",
    "runs = storage.list_runs(limit=20)\n",
    "\n",
    "# Convert to DataFrame\n",
    "runs_df = pd.DataFrame(runs)\n",
    "runs_df['timestamp'] = pd.to_datetime(runs_df['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(runs_df)} runs\")\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Metrics by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract aggregate metrics\n",
    "metrics_rows = []\n",
    "for _, row in runs_df.iterrows():\n",
    "    metrics = row['aggregate_metrics']\n",
    "    metrics['run_id'] = row['run_id']\n",
    "    metrics['model'] = row['model_name']\n",
    "    metrics['task'] = row['task_name']\n",
    "    metrics['timestamp'] = row['timestamp']\n",
    "    metrics_rows.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Accuracy by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for accuracy metric\n",
    "if 'accuracy' in metrics_df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Bar plot by model\n",
    "    model_accuracy = metrics_df.groupby('model')['accuracy'].mean().sort_values(ascending=False)\n",
    "    model_accuracy.plot(kind='bar')\n",
    "    \n",
    "    plt.title('Average Accuracy by Model', fontsize=14)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No accuracy metric found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison (JSON Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter JSON task results\n",
    "json_results = metrics_df[metrics_df['task'] == 'json']\n",
    "\n",
    "if len(json_results) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    if 'accuracy' in json_results.columns:\n",
    "        json_results.groupby('model')['accuracy'].mean().plot(kind='bar', ax=axes[0])\n",
    "        axes[0].set_title('JSON Extraction Accuracy')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Parse rate comparison\n",
    "    if 'parse_rate' in json_results.columns:\n",
    "        json_results.groupby('model')['parse_rate'].mean().plot(kind='bar', ax=axes[1])\n",
    "        axes[1].set_title('JSON Parse Success Rate')\n",
    "        axes[1].set_ylabel('Parse Rate')\n",
    "        axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No JSON task results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "exporter = CSVExporter(storage)\n",
    "\n",
    "# Export summary\n",
    "num_exported = exporter.export_summary('../data/exports/summary.csv', limit=20)\n",
    "print(f\"Exported {num_exported} runs to summary.csv\")\n",
    "\n",
    "# Export individual run details (optional)\n",
    "# if len(runs) > 0:\n",
    "#     latest_run = runs[0]['run_id']\n",
    "#     exporter.export_run(latest_run, f'../data/exports/{latest_run}.csv')\n",
    "#     print(f\"Exported detailed results for {latest_run}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample-level results for a specific run\n",
    "if len(runs) > 0:\n",
    "    latest_run_id = runs[0]['run_id']\n",
    "    samples = storage.get_sample_results(latest_run_id)\n",
    "    \n",
    "    samples_df = pd.DataFrame(samples)\n",
    "    \n",
    "    print(f\"Latest run: {latest_run_id}\")\n",
    "    print(f\"Total samples: {len(samples_df)}\")\n",
    "    \n",
    "    # Extract scores\n",
    "    scores_df = pd.DataFrame(samples_df['scores'].tolist())\n",
    "    \n",
    "    print(\"\\nScore distributions:\")\n",
    "    print(scores_df.describe())\n",
    "else:\n",
    "    print(\"No runs available for sample analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
